Q: How is a grayscale image represented on a computer? How about a color image?
- Grayscale: 2 rank tensor
- Color: 3 rank tensor

Q: How are the files and folders in the `MNIST_SAMPLE` dataset structured? Why?
A:
    - train
    - valid

Q: Explain how the "pixel similarity" approach to classifying digits works.
A: 
-  This method involves comparing the pixel values of an unknown digit image to a set of known digit images and determining the closest match based on some measure of similarity.

Q: What is a "rank-3 tensor"?
A:
- A 3D matrix --> 3 dimensions

Q: What is the difference between tensor rank and shape? How do you get the rank from the shape?
A:
- Tensor rank: The number of dimensions in a matrix 
- Tensor shape: The size of each dimension in a matrix
Example: 1x3 matrix : [2, 1, 2] --> this is a rank-1-tensor since its a one-dimensional array and the shape is (1, 3) because ther are three elements in this one-dimensional array


Q: What are RMSE and L1 norm?
A:
- Two commonly used metrics for measuring the difference between values predicted by a model and the values actually observed.
    1. RMSE is the square root of the average of the squares of the errors. The error is the amount by which the prediction deviates from the actual values.
    2. The L1 norm is the sum of the absolute differences between the predicted values and the actual values. 

Q: What is broadcasting?
A:
- Smaller rank are expanded to have the same size as larger rankes to allow operations to be performed on arays of different shapes

Q: Are metrics generally calculated using the training set, or the validation set? Why?
A:
- Validation set

Q: What is SGD?
A:
- Stochastic gradient descent: An optimizer that adjusts weights in a deep neural network with the goal of minimizing the networks loss.
    For each training step, SDG randomly select a mini-batch and computes an estimate of the gradient. 

Q: Why does SGD use mini-batches?
A:
- The randomness in mini-batch selection helps avoid local minima and promotes better generalization in the model.

Q: What are the seven steps in SGD for machine learning?
A:
1. Randomly initlize the parameters of the model
2. Get predictions
3. Choose and calculate the loss
4. Calculate the gradiants of the loss function with respect to the models parameters
5. Update the parameters
6. Repeat steps 2-5 for each epoch
7. Evaluate the model and adjust hyperparameters


Q: How do we initialize the weights in a model?
A: 
 ````params = torch.randn(3).requires_grad_()````

Q: What is "loss"?
A:
- The models performance. Acutal vs. observed predictions

Q: Why can't we always use a high learning rate?
A:
1. Too low: The model takes small steps and will eventually learn, but it will take a very long time. 
    It might even et stuck in a local minima.
2. Too high: The model might overshoot the optimal solution and never converge. The loss might increase with each step. 

Q: What is a "gradient"?
A:
- A gradiant is a vector that points to the greatest rate of increse of a function.
    For a function with multiple inputs, like the loss function, the gradiant is a vector that contains the partial
    derivaties of the function with respect to each input. Each partial derivative tells you how much the output
    of the function changes if you change a single input while keeping the others constant.

Q: Do you need to know how to calculate gradients yourself?
A:
- No, frameworks like fastai and pytorch calculate gradiants for you. They have built-in automatic differentiation capabilities.
Q: Why can't we use accuracy as a loss function?
A:
- Accuracy is a discrete metric (either correct or incorrect) and therefore not differantiable. 
This means we cannot calculate the gradiants of accuracy with respect to the model parameters.

Q: What is the difference between a loss function and a metric?
A:
- The loss is used for calculating the models performance
- A metric is used for calculating the models performance on a spesific task

Q: What is the function to calculate new weights using a learning rate?
A:
- ````new_weight = old_weight - (learning_rate * gradient)````
    1. old_weight: This is the current value of the weight that we want to update.
    2. learning_rate: This hyperparameter controls the step size in the weight update. A smaller learning rate means smaller steps, and a larger learning rate means larger steps.
    3. gradient: This is the partial derivative of the loss function with respect to the weight. 
    It tells us the direction and magnitude of the steepest ascent of the loss function. We want to move in the opposite direction of the gradient to minimize the loss.

Q: Write pseudocode showing the basic steps taken in each epoch for SGD.
A:

# Initialize model parameters (weights and biases)

for epoch in range(num_epochs):
    Shuffle the training dataset

    for batch in batches_of_training_data:

        # 1. Forward Pass:
        Calculate model predictions for the current batch of data.

        # 2. Calculate Loss:
        Compare model predictions to the true labels and compute the loss function (e.g., Mean Squared Error, Cross-Entropy).

        # 3. Backpropagation:
        Compute gradients of the loss with respect to each model parameter using automatic differentiation (e.g., backpropagation).

        # 4. Update Parameters:
        for each parameter:
            new_parameter = old_parameter - (learning_rate * gradient)

# End of epoch

Q: What are the "bias" parameters in a neural network? Why do we need them?
A:
- What are they:
    1. Extra Neuron: In essence, bias parameters act like an extra neuron in each layer of a neural network (except the output layer).
    2. Constant Input: This extra neuron has a constant input of 1.
    3. Trainable Weight: The bias neuron has its own weight, just like the connections between other neurons. This weight is learned during training.
    The output of a neuron is calculated as:
    ```output = activation_function(w1*x1 + w2*x2 + ... + wn*xn + b)```

- Why do we need them?
    1. Shifting the Activation Function: The bias parameter allows us to shift the activation function to the left or right along the input axis. This is crucial because it gives the neuron more flexibility to fit the data
    2. Modeling Complex Relationships: The bias helps the model capture complex relationships between the inputs and outputs. It allows the neuron to fire even when all the inputs are zero, which is necessary for certain types of patterns in the data.

Q: What does the `backward` method do?
A:
- The primary purpose of the backward() method is to calculate the gradients of the loss function with respect to all the parameters of the neural network.

Q: Why do we have to zero the gradients?
A:
- Deep learning frameworks like PyTorch accumulate gradients by default. This means that every time you call the .backward() method on a loss, the gradients calculated for each parameter are added to the existing gradients stored in the parameter's .grad attribute.
If you don't zero out the gradients before each batch, the gradients from previous batches will interfere with the gradients calculated for the current batch. This can lead to incorrect weight updates and prevent the model from learning effectively.

Q: What information do we have to pass to `Learner`?
A:
- DataLoader objekt
- Model objekt
- Metric

Q: Show Python or pseudocode for the basic steps of a training loop.
A:
```
model = YourMode()
loss_fn = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SDG(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    model.train()
    for batch, data , target in enumerate(train_loader):
        optimizer.zero_grad()
        ouputs = model(data)
        loss = loss_fn(output, target)
        loss.backward()
        optimzer.step()     

```
Q: What is "ReLU"? Draw a plot of it for values from `-2` to `+2`.
A:
- ReLU is an activation function defined as:
```ReLU(x) = max(0, x)```
This means that for any input value x:
If x is negative or zero, the output is 0.
If x is positive, the output is x.

Q: What is an "activation function"?
A:
- Functions applied to the output of a neuron. They introduce non-linearity into the network, which is essential for the network to learn complex patterns and relationships in data.
- It helps define decision bounderies that a neural netowrk uses to classify or predict. 
-Mimics the way a biologicla neuron fire or don't based on the input sinals it recieves.