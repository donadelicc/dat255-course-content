{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2db3b5",
   "metadata": {},
   "source": [
    "A.S. Lundervold, 29.01.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef348a8-18cc-431b-b6d8-0970f7c2f9cc",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The building blocks of neural networks, Part 1: Tensors and tensor operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70642d1-b27c-4588-8539-05d6c08daacf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> This two-part tutorial is meant to increase your familiarity with the basics of PyTorch and the basic building blocks of artificial neural networks. \n",
    "\n",
    "> This notebook is partly based on Chapter 2 of Chollet's text book \"Deep learning with Python\", 2nd edition: https://livebook.manning.com/book/deep-learning-with-python-second-edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ef86b",
   "metadata": {},
   "source": [
    "> Note that this notebook is accompanying what we did in the lectures, and is not complete in itself. It is meant as a reference for you to look at later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f189ef-1112-481a-9b36-5a4c5d774bc4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As deep neural networks consist of a set of chained operations on what's called _tensors_, we'll take a closer look at _tensors_ and _tensor operations_. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e120ee-0d54-4bf4-b17a-ef3ab4097ca6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Plan:**\n",
    "\n",
    "1. Define tensors\n",
    "2. Vocabulary and examples\n",
    "3. Tensor operations\n",
    "4. A quick linear algebra refresher: geometric interpretations of tensor operations\n",
    "\n",
    "\n",
    "**Takeaway**:\n",
    "\n",
    "> Our main takeaway will be that **deep neural networks can be viewed as a long chain of geometric transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69478538-e765-4152-84c5-ac05bf146039",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0bfe04a-7650-4008-8f0c-e4eb0d4ba67f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is a quick check of whether the notebook is currently running on Google Colaboratory\n",
    "# or on Kaggle, as that makes some difference for the code below.\n",
    "try:\n",
    "    import colab\n",
    "    colab=True\n",
    "except:\n",
    "    colab=False\n",
    "\n",
    "import os\n",
    "kaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307ef235-68a7-4b79-aeba-e9510a95cc62",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (colab or kaggle):\n",
    "    !pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fb68af-895d-4927-b784-ec9764775c9a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np, matplotlib.pyplot as plt, pandas as pd, sklearn.datasets\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf29b4bd-5fd2-4501-b5ff-5effe84da63c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Set up data directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fadf9744-bb11-4bd2-a220-4b914e3fe87b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NB_DIR = Path.cwd()\n",
    "# Change this if you want to store the images that are downloaded\n",
    "# below elsewhere on your computer.\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "    DATADIR = Path(\"/content/gdrive/MyDrive/Colab Notebooks/data\")\n",
    "    DATADIR.mkdir(exist_ok=True)\n",
    "if not colab:\n",
    "    DATADIR = Path.home()/'data'\n",
    "    DATADIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0f18ee9-c797-4144-8bda-206d087a19c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b6296-5a5a-46a2-9e34-82c38f4c2a7a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92dc3558-af5d-402a-bea8-8fa6bfcb7b76",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 14654355.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\prebe\\data\\MNIST\\raw\\train-images-idx3-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 14305112.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\prebe\\data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 11694635.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\prebe\\data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 2224489.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\prebe\\data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to C:\\Users\\prebe\\data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to C:\\Users\\prebe\\data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:13<00:00, 12993458.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\prebe\\data\\cifar-10-python.tar.gz to C:\\Users\\prebe\\data\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mnist = torchvision.datasets.MNIST(root=DATADIR, train=True, download=True, transform=transform)\n",
    "cifar = torchvision.datasets.CIFAR10(root=DATADIR, train=True, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84eabda-a7ae-479f-bc1b-937ade543ade",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "housing = sklearn.datasets.fetch_california_housing()\n",
    "\n",
    "housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f5afe-9d57-4a3b-aa39-8d80ba80de07",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d2314-28c6-43d8-871b-f28430f63758",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tensors are multidimensional arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054eeb66-a2e5-4ce4-b790-340650fac370",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Vocabulary: the rank of a tensor\n",
    "\n",
    "The **rank** of a tensor, often referred to as its number of dimensions in PyTorch (denoted as `ndim`), represents the number of axes it has. For example, a matrix with rows and columns is a rank-2 tensor.\n",
    "\n",
    "The **shape** of a tensor indicates the size along each of its axes. For instance, a tensor with a shape of (3, 2) signifies 3 elements along one axis (rows) and 2 along the other (columns).\n",
    "\n",
    "A tensor's **data type** refers to the type of data stored in the tensor. Unlike general-purpose arrays, tensors require a uniform data type for all elements. This uniformity, particularly when coupled with GPUs or other accelerators, significantly enhances the efficiency of linear algebra computations, as it optimizes memory usage and allows for more effective parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3939ce2-eac2-4c67-b6f5-db975f033e5c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Rank 0 tensors: scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f683707-44d4-427a-950f-e832eae35eff",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rank 0 tensors stores scalars (integers or floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15753a2a-336d-4e23-b55c-c7f5248c2485",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = 42\n",
    "\n",
    "tns = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e072761-20a6-40c7-964c-d65ebba4b78a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa0fca1-e311-4da8-bfa5-c00b9807adb9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c4cfe38-1c5f-4554-825f-4488dad3e283",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0331e80-107d-4e1d-ba8e-98dfa22b3885",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7fc56b-8e92-4d10-9acb-b1a22a22b73b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Rank 1 tensors: vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1aa239-48e9-4742-9465-cc9e80455012",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rank 1 tensors are _vectors_ or _arrays of numbers_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d5f64e-038c-4808-a973-dfc49492bc55",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = [12,13,14]\n",
    "\n",
    "tns = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32962029-782b-4318-a056-6f2a7dfa6dc0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 14])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70ecf66b-62f3-4abc-9674-5b44b7f97977",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "541ce33a-4277-44c4-b301-0ecb77a7a776",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9c1fe57-4aaf-424a-951a-f8b9e8dda5cc",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad922cca-f2d3-4732-bcc9-6b1dd74de88d",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> **Warning:** When speaking about vectors, the **dimension of a vector** is the number of entries in the vector. This can be a bit confusing. The number of dimensions of the vector [1,2,3] is 3, while its dimension as a tensor is 1. It is therefore safer to use the word **rank** when referring to the tensor dimension (as in, [1,2,3] is a rank 1 tensor). In other words, it has only one **axis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a1e4b-a355-45fe-a9ab-5f1037329a56",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956f611-a987-4080-b3c3-0bf1267fa755",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You'll often deal with rank 1 tensors. For example, each instance of a tabular data set can be represented as vectors containing all the corresponding feature values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aae55c7d-17fc-4b4c-a344-61526b991400",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.02381</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127    1.02381       322.0  2.555556     37.88   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338095d1-6889-43e1-8558-8897646e5878",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Rank 2 tensors: matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46750a19-8c6e-4289-86b4-734730ee1179",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Rank 2 tensors are what corresponds to standard 2D matrices or arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2209a24b-4d0f-4dc3-9567-1f14837ccdf1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = [[12,13,14], [15,16,17]]\n",
    "\n",
    "tns = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "634f647e-8fb8-46e7-84ef-433048d945b3",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 13, 14],\n",
       "        [15, 16, 17]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d97624ec-74fa-4219-af68-96946ee0a40c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62d5977f-441c-4227-837e-01c8a132fb03",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aaa1ba05-b028-49be-a9f0-3046ca1ea3e5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb8f14-e0f9-47d4-a52c-ce1ee1a99764",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719c9042-c3e4-41a2-a944-22a377c4e29a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A common way to end up with rank 2 tensors in machine learning is as representation of tabular data sets. Each data instance consists of a vector (rank 1 tensor) containing feature values (think price, color, age, etc), and a batch of data is a number of such instances. \n",
    "\n",
    "You end up with a matrix where the first axis is the sample axis and the second axis is the feature axis: (`samples`, `features`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ffda99f-21ea-478b-933d-225c8ad265c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dcd6c235-271f-476b-a2fd-603de7d8ecd0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tns = torch.tensor(housing.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "604ff809-3c2e-4721-9836-6939fe341cde",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   8.3252,   41.0000,    6.9841,  ...,    2.5556,   37.8800,\n",
       "         -122.2300],\n",
       "        [   8.3014,   21.0000,    6.2381,  ...,    2.1098,   37.8600,\n",
       "         -122.2200],\n",
       "        [   7.2574,   52.0000,    8.2881,  ...,    2.8023,   37.8500,\n",
       "         -122.2400],\n",
       "        ...,\n",
       "        [   1.7000,   17.0000,    5.2055,  ...,    2.3256,   39.4300,\n",
       "         -121.2200],\n",
       "        [   1.8672,   18.0000,    5.3295,  ...,    2.1232,   39.4300,\n",
       "         -121.3200],\n",
       "        [   2.3886,   16.0000,    5.2547,  ...,    2.6170,   39.3700,\n",
       "         -121.2400]], dtype=torch.float64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2cf23aa3-c9fa-4ce8-8774-f3dafaa4c8f7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20640, 8])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71312739-1cce-450f-961e-de38acf88160",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72815551-14f8-4e92-97bd-41466904076b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97eda32-6a29-4a57-951f-c33e59920483",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another way to end up with rank 2 tensors are time series or sequence data: (`timesteps`,`features`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffad0b3-cc6e-4d95-ad62-e2104635f9b6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Tensors of rank 3 and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f29140-9db0-44bf-8ff1-717c6bcf1b23",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you stack several rank 2 tensors you'll obtain a rank 3 tensor. If you stack rank 3 tensors, you'll have a rank 4 tensor. And so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08372625-d36f-4032-b0e9-aa2d78450977",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tns1 = torch.tensor(\n",
    "            [[1,2,3], [4,5,6]]\n",
    "        )\n",
    "\n",
    "tns2 = torch.tensor(\n",
    "            [[7,8,9], [10,11,12]]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7aabd28-2552-42d5-bb68-952aa6519f8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a711f76-bbc2-4bd5-a699-62c4e25da667",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "702c8ae5-003d-42dc-8adc-8ad053d0ec28",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns1.ndim, tns2.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6f4d4df-cec9-4188-b317-e2e0b975d10b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tns = torch.stack((tns1, tns2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "597f62b0-311a-42ce-a89e-a6d45cfd7ba0",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1,  2,  3],\n",
       "         [ 4,  5,  6]],\n",
       "\n",
       "        [[ 7,  8,  9],\n",
       "         [10, 11, 12]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f39f313-872a-4f85-8d2b-170a2f4f922c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf7905b5-ab82-4e74-8979-a386bb9e91f6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cf771f4-d24e-4cda-8d05-08ed5eaa61af",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tns4 = torch.stack((tns, tns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "890e80bf-973d-41e4-bdb6-1fd37df1c419",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns4.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d686e-3bdd-49b7-a38f-c479f7578efd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8583b-4aaa-4350-be4e-d3cad9fab643",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you're dealing with **time series or sequence data** (i.e., where each instance is a rank 2 tensor (`timesteps`,`features`), you'll end up with rank 3 tensors: (`samples`, `timesteps`, `features`). \n",
    "\n",
    "Individual **images** are also typically represented as rank 3 tensors. The three axes of an image tensor is height, width and channel (typically color channel): (`height`, `width`, `channels`). For color images, the three channels are typically R, G and B. For grayscale images one typically inserts a single channel axis.\n",
    "\n",
    "If you're dealing with image data consisting of multiple images then you'll typically represent your data as rank 4 tensors: (`samples`, `height`, `width`, `channels`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b244ae7f-25a5-4386-be8a-dc28ea63323e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9831f57-b2e9-4727-a126-3b99c6ed9f4c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar.data[0].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6aa990d5-5b1a-433c-bc39-6009ec6f9707",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8266b067-0061-4b35-974e-fbd1e5409d99",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mnist_example = mnist.data[0].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fd23c1de-214c-4c09-b991-56b04bc3e01c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87720284-6cb3-4191-af14-661833369350",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_example.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f287f414-2b43-4eae-8b10-425dd4dd6acd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A batch of images will then be a rank 4 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d8056c9-38a4-4bc7-bd48-36c277454250",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "912681ed-6b24-4d62-90bd-14f29cc3bd3f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar.data.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22880b44",
   "metadata": {},
   "source": [
    "#### 3D images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb69346",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c5/MRI_brain_sagittal_section.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38543c9-e534-45ed-b912-c30a45b2f018",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03aeb59-2abf-4fda-a10d-3be409fdbcfb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A video is a series of image frames. As the images are rank 3 tensors, a video can be represented as a rank 4 tensor by stacking the frames. A batch of videos will then be a rank 5 tensor: \n",
    "\n",
    "`(samples, frames, height, width, color_depth)` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7eb31b-9209-49a3-8809-1ea285ed2212",
   "metadata": {},
   "source": [
    "# Tensor operations and linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69049a7-e1e4-4952-9395-3a61d145618f",
   "metadata": {},
   "source": [
    "All the operations in a deep neural network are based on a few simple tensor operations, like addition, multiplication and simple nonlinear functions applied to tensors. \n",
    "\n",
    "Since tensors are multidimensional arrays. Therefore, **linear algebra** is at the heart of deep learning. As linear algebra is very **geometric**, this gives us a geometric point of view for deep learning. \n",
    "\n",
    "We discussed this in class. Here's a simple example: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac29ad63",
   "metadata": {},
   "source": [
    "## Example: matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343993d2",
   "metadata": {},
   "source": [
    "Consider a neural network layer transforming a 2D input $(x, y)$ to another 2D output. This transformation can be represented by multiplying the input vector by a 2x2 matrix $W$ (the weights of the layer).\n",
    "\n",
    "Given an input vector $[x, y]$ and a weight matrix $W = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix}$, the output is computed as:\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} w_{11} \\cdot x + w_{12} \\cdot y \\\\ w_{21} \\cdot x + w_{22} \\cdot y \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This operation can be understood as transforming the 2D input in a new space defined by the weight matrix $W$. The transformation can include operations like rotation, scaling, and shearing, determined by the values of $w_{11}, w_{12}, w_{21},$ and $w_{22}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd6a79",
   "metadata": {},
   "source": [
    "Here's a visualization of this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e437b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e87ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vectors(input_vector, transformed_vector, weight_matrix_str):\n",
    "    \"\"\"Plot the original and transformed vectors.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot([0, input_vector[0]], [0, input_vector[1]], 'bo-', label='Original Vector')\n",
    "    plt.plot([0, transformed_vector[0]], [0, transformed_vector[1]], 'ro-', label='Transformed Vector')\n",
    "    plt.xlim(-10, 10)\n",
    "    plt.ylim(-10, 10)\n",
    "    plt.axhline(0, color='grey', lw=1)\n",
    "    plt.axvline(0, color='grey', lw=1)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.text(1, 5, weight_matrix_str, fontsize=12, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    plt.show()\n",
    "\n",
    "def plot_transformed_point(w11, w12, w21, w22, input_x, input_y):\n",
    "    \"\"\"Calculate the transformed vector and plot it.\"\"\"\n",
    "    weight_matrix = np.array([[w11, w12], [w21, w22]])\n",
    "    input_vector = np.array([input_x, input_y])\n",
    "    transformed_vector = np.dot(weight_matrix, input_vector)\n",
    "    weight_matrix_str = f\"Weight Matrix:\\n[[{w11:.2f}, {w12:.2f}]\\n [{w21:.2f}, {w22:.2f}]]\"\n",
    "    plot_vectors(input_vector, transformed_vector, weight_matrix_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d89663e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_slider(description, value=1.0, min=-2.0, max=2.0, step=0.1):\n",
    "    \"\"\"Create a slider with the given properties.\"\"\"\n",
    "    return widgets.FloatSlider(value=value, min=min, max=max, step=step, description=description)\n",
    "\n",
    "# Create a dictionary to store the sliders\n",
    "sliders = {\n",
    "    'w11': create_slider('Weight w11:'),\n",
    "    'w12': create_slider('Weight w12:', value=0.0),\n",
    "    'w21': create_slider('Weight w21:', value=0.0),\n",
    "    'w22': create_slider('Weight w22:'),\n",
    "    'input_x': create_slider('Input x:', min=-5.0, max=5.0),\n",
    "    'input_y': create_slider('Input y:', min=-5.0, max=5.0),\n",
    "}\n",
    "\n",
    "# Assembling the UI\n",
    "ui = widgets.VBox(list(sliders.values()))\n",
    "out = widgets.interactive_output(plot_transformed_point, sliders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a91b6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4efc6276bdb4b67944355d0b9ccb1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FloatSlider(value=1.0, description='Weight w11:', max=2.0, min=-2.0), FloatSlider(value=0.0, de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d36275c90c3497ea6543556c2bb5138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(outputs=({'output_type': 'display_data', 'data': {'text/plain': '<Figure size 600x600 with 1 Axes>', 'i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48866cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT225-venv",
   "language": "python",
   "name": "dat225-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
